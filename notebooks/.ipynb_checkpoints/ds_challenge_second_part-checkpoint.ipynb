{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tb.lx Data Science Challenge - Part II\n",
    "----\n",
    "----\n",
    "## Introduction\n",
    "\n",
    "Dear applicant,\n",
    "\n",
    "Congratulations on passing the first screening! We’re excited to get to know you better and get a better feeling of your competences. In this round, we will test you on your problem-solving skills and data science experience by giving you a case to solve.\n",
    "\n",
    "After handing us over your solution, we will review it and let you know our feedback. In the case you have passed, you will be called to an on-site interview. During the interview, you’ll get the opportunity to explain your solution and the steps that you took to get there. We've prepared this notebook for you, to help you walk us through your ideas and decisions.\n",
    "\n",
    "If you're not able to fully solve the case, please elaborate as precisely as you can:\n",
    "\n",
    "- Which next steps you'd be taking;\n",
    "- Which problems you'd be foreseeing there and how you'd solve those.\n",
    "\n",
    "In case you have any questions, feel free to contact ana.cunha@daimler.com or sara.gorjao@daimler.com for any more info. \n",
    "\n",
    "Best of luck!\n",
    "\n",
    "## Context:\n",
    "\n",
    "Predictive Maintenance is one of the hottest topics in the heavy-industry field. The ability to detect failures before they happen is of utmost importance, as it enables the full utilization of materials saving in unnecessary early replacements, and enables optimizations in maintenance planning reducing the downtime.\n",
    "\n",
    "\n",
    "## Data:\n",
    "\n",
    "One of the challenges in the auto-tech industry is to detect failures before they happen. For this, we included a dataset including:\n",
    "* `telemetry.csv`: Consists of a dataset with sensor values along time\n",
    "* `faults.csv`: Consists of a dataset with faults for each machine along time.\n",
    "* `errors.csv`: Consists of a dataset with errors for each machine along time.\n",
    "* `machines.csv`: Consists of a dataset with features for each machine. \n",
    "\n",
    "\n",
    "## Task:\n",
    "\n",
    "In the second part of the challenge, we would like to know that a failure is going to happen before it actually happens. The decision of the prediction horizon is totally up to you, **but the goal is to predict failures before they happen**.\n",
    "\n",
    "\n",
    "## Questions:\n",
    "\n",
    "Follows a set of theoretical questions:\n",
    "\n",
    "1. How can you create a machine learning model that leverages all the data that we provided whilst adapting to the specificities of each turbine (e.g., operating in different weather conditions)?\n",
    "2. Modeling the normal behaviour of such machines can prove itself to be a good feature. After training a model that captures the normal turbine dynamics, we need to decide when the displayed behaviour may be considered an anomaly or not. How can one design a framework that creates alerts for abnormality without overloading the end-user with too many false positives?\n",
    "3. How would you measure aleatoric uncertainty of the predictions of your model?\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "- Solution implemented in Python3.6+;\n",
    "- Provide requirements.txt to test the solution in the same environment;\n",
    "- Write well structured, documented, maintainable code;\n",
    "- Write sanity checks to test the different steps of the pipeline;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isto aqui vai ser muito como as coisas que ja tenho visto de TTF. Load datasets ver o RUL ver quantos time-steps faltam\n",
    "# até o RUL e por ai fora\n",
    "#https://www.kaggle.com/nafisur/predictive-maintenance-using-lstm-on-sensor-data\n",
    "#https://www.kaggle.com/billstuart/predictive-maintenance-ml-iiot\n",
    "#https://www.kaggle.com/hanwsf8/lstm-lgb-catb-for-predictive-maintenance-upper\n",
    "#https://www.kaggle.com/juhumbertaf/tutorial\n",
    "#https://iopscience.iop.org/article/10.1088/1742-6596/1037/6/062003/pdf\n",
    "#https://www.kaggle.com/c/equipfails/overview\n",
    "#https://www.kaggle.com/uciml/aps-failure-at-scania-trucks-data-set\n",
    "#https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/predictive-maintenance-playbook#data-science-for-predictive-maintenance\n",
    "#https://github.com/Azure-Samples/MachineLearningSamples-DeepLearningforPredictiveMaintenance\n",
    "#https://gallery.azure.ai/Notebook/Predictive-Maintenance-Implementation-Guide-R-Notebook-2\n",
    "#https://gallery.azure.ai/Collection/Predictive-Maintenance-Template-3\n",
    "\n",
    "# Para a primeira pergunta: Dizer algo como garantir que o modelo não esta a fazer overfitting de maneira a conseguir\n",
    "# adaptar-se a novas turbinas (também posso dizer \"garantir que os dados são representativos do que queremos\")\n",
    "\n",
    "# Para a segunda pergunta: Fazer one class classification\n",
    "\n",
    "# Para a terceira pergunta: algo como bayesian estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the datasets\n",
    "telemetry = pd.read_csv(\"../data/sensor/telemetry.csv\", index_col=0)\n",
    "failures = pd.read_csv(\"../data/sensor/failures.csv\")\n",
    "errors = pd.read_csv(\"../data/sensor/errors.csv\")\n",
    "machines = pd.read_csv(\"../data/sensor/machines.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting datetime strings to datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry[\"datetime\"] = pd.to_datetime(telemetry[\"datetime\"])\n",
    "failures[\"datetime\"] = pd.to_datetime(failures[\"datetime\"])\n",
    "errors[\"datetime\"] = pd.to_datetime(errors[\"datetime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the additional information\n",
    "\n",
    "Since we have other relevant information scattered on different dataframes, it is useful to  join these dataframes into one containing all the needed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>machineID</th>\n",
       "      <th>volt</th>\n",
       "      <th>rotate</th>\n",
       "      <th>pressure</th>\n",
       "      <th>vibration</th>\n",
       "      <th>failures</th>\n",
       "      <th>errorID</th>\n",
       "      <th>model</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 06:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>176.217853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113.077935</td>\n",
       "      <td>45.087686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no_errors</td>\n",
       "      <td>model3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 07:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>162.879223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.460525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no_errors</td>\n",
       "      <td>model3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 08:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>527.349825</td>\n",
       "      <td>75.237905</td>\n",
       "      <td>34.178847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no_errors</td>\n",
       "      <td>model3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>162.462833</td>\n",
       "      <td>346.149335</td>\n",
       "      <td>109.248561</td>\n",
       "      <td>41.122144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no_errors</td>\n",
       "      <td>model3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 10:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>157.610021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.886648</td>\n",
       "      <td>25.990511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no_errors</td>\n",
       "      <td>model3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  machineID        volt      rotate    pressure  \\\n",
       "0 2015-01-01 06:00:00          1  176.217853         NaN  113.077935   \n",
       "1 2015-01-01 07:00:00          1  162.879223         NaN   95.460525   \n",
       "2 2015-01-01 08:00:00          1         NaN  527.349825   75.237905   \n",
       "3 2015-01-01 09:00:00          1  162.462833  346.149335  109.248561   \n",
       "4 2015-01-01 10:00:00          1  157.610021         NaN  111.886648   \n",
       "\n",
       "   vibration  failures    errorID   model  age  \n",
       "0  45.087686       0.0  no_errors  model3   18  \n",
       "1        NaN       0.0  no_errors  model3   18  \n",
       "2  34.178847       0.0  no_errors  model3   18  \n",
       "3  41.122144       0.0  no_errors  model3   18  \n",
       "4  25.990511       0.0  no_errors  model3   18  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining the newly added failures column to the telemetry dataframe\n",
    "failures[\"failures\"] = 1\n",
    "telemetry = telemetry.merge(failures, on=[\"machineID\", \"datetime\"], how=\"left\")\n",
    "telemetry[\"failures\"].fillna(0, inplace=True)\n",
    "\n",
    "# Joining the errors columns to the telemetry dataframe\n",
    "telemetry = telemetry.merge(errors, on=[\"machineID\", \"datetime\"], how=\"left\")\n",
    "telemetry[\"errorID\"].fillna(\"no_errors\", inplace=True)\n",
    "\n",
    "# Joining the static machine information to the telemetry dataframe\n",
    "telemetry = telemetry.merge(machines, on=\"machineID\", how=\"left\")\n",
    "telemetry.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a complete dataset with time-series data and static data to use. We can see there are some missing values present in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime          0\n",
       "machineID         0\n",
       "volt         139548\n",
       "rotate       139597\n",
       "pressure     139461\n",
       "vibration    139558\n",
       "failures          0\n",
       "errorID           0\n",
       "model             0\n",
       "age               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "telemetry.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing data\n",
    "\n",
    "Nearly 16% of the `volt`, `rotate`, `pressure` and `vibration` columns are missing which is a good portion of the data. Given the size of the missing data it discards the possibility of simply dropping rows with missing values in them, as that would have too much of an impact in the quality of our data.\n",
    "\n",
    "What we can do instead is to impute the missing values with a value that makes sense. This value can be the mean, mode, median, etc. of these columns. However different machines might have different mean/median/mdode values for these columns so it is imporant to impute these values with consideration about which machine is being imputed.\n",
    "\n",
    "Since the missing values are in the dynamic data columns, we can go one step further than imputting with just the mean for example. Since these values change over time we can impute the missing values with a linea interpolation of the previous points in order to minimize the disruption in the data, giving us the best possible value for our missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df, group_column, column_name, interpolate=True):\n",
    "    \"\"\"\n",
    "        Fills the missing values in column_name by either the interpolated value (if interpolate is True)\n",
    "        or the mean of the taken from from each group in group_column\n",
    "        \n",
    "        Arguments:\n",
    "            df: The dataframe to update\n",
    "            group_column: The name of the column by which to group the data\n",
    "            column_name: The name of the column in which to replace the missing values\n",
    "            interpolate: Boolean flag indicating if the missing values should be imputed with the interpolated\n",
    "                value or by the mean\n",
    "                \n",
    "        Returns:\n",
    "            A pd.Series object with missing value replaced by a meaningful value, to replace the original column\n",
    "            in the dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    if interpolate:\n",
    "        return df.groupby(group_column)[column_name].apply(lambda x: x.fillna(x.interpolate(method='linear')))\n",
    "    else:\n",
    "        return df.groupby(group_column)[column_name].apply(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "\n",
    "for column in [\"volt\", \"rotate\", \"pressure\", \"vibration\"]:\n",
    "    \n",
    "    # Impute missing values with the interpolated values\n",
    "    telemetry[column] = fill_missing_values(telemetry, \"machineID\", column)\n",
    "    \n",
    "    # Some missing values may still persists (in cases where interpolation is not possible) for those \n",
    "    # we'll impute with the mean\n",
    "    telemetry[column] = fill_missing_values(telemetry, \"machineID\", column, interpolate=False)\n",
    "    \n",
    "# Checking the missing values\n",
    "telemetry.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, no more missing values in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding information about the cycle of each machine\n",
    "telemetry[\"cycle\"] = telemetry.groupby(\"machineID\").cumcount()\n",
    "\n",
    "def get_failure_cycles():\n",
    "    \"\"\"\n",
    "        Gets a hash-map (a python dictionary) of all the cycles where a fail occured for each machine, for faster lookup times\n",
    "        Hash-Map format {machineID: <List of cycles where fail occured>}\n",
    "        \n",
    "        Returns:\n",
    "            A hash-map containing a list of cycles where each machine failed\n",
    "    \"\"\"\n",
    "    \n",
    "    hash_map = {}\n",
    "    \n",
    "    # Iterate over every machineID\n",
    "    for machine_id in telemetry[\"machineID\"].unique():\n",
    "        \n",
    "        # Get the list of cycles where a failure occured for this machine\n",
    "        failure_cycles = telemetry.loc[(telemetry[\"machineID\"] == machine_id) & (telemetry[\"failures\"] == 1), \"cycle\"].to_numpy()\n",
    "        \n",
    "        # Insert new key and new value to hash-map\n",
    "        hash_map[machine_id] = np.unique(failure_cycles)\n",
    "        \n",
    "    return hash_map\n",
    "\n",
    "# hash-map containing every failure cycles for all machines\n",
    "failure_cycles = get_failure_cycles()\n",
    "\n",
    "def add_time_to_failure(row, failure_cycles):\n",
    "    \"\"\"\n",
    "        Calculates the number of cycles left until a failure is observed, \n",
    "        based on the currect cycle of the current machine.\n",
    "        \n",
    "        The observations between the last known failure and the end of the analysis have a common time-to-failure\n",
    "        of 9999 since these observations are censored and a failure is not garanteed.\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "        Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the list of failure cycles for this machine\n",
    "    machine_failure_cycles = failure_cycles[row[\"machineID\"]]\n",
    "    \n",
    "    # If the current cycle is a failure cycle, then return 0 (there are 0 cycles until a failure is observed)\n",
    "    if row[\"cycle\"] in machine_failure_cycles:\n",
    "        return 0\n",
    "    \n",
    "    # Get the index in which the current cycle of this machine would be added\n",
    "    # in the list of failures for this machine. This index holds the value of of the next failure cycle\n",
    "    # Example: \n",
    "    #    arr = [96, 150, 300]\n",
    "    #    current_cycle = 50\n",
    "    #    bisect.bisect(arr, current_cycle) \n",
    "    #    >> 0\n",
    "    #\n",
    "    #    current_cycle = 200\n",
    "    #    bisect.bisect(arr, current_cycle) \n",
    "    #    >> 2\n",
    "    next_failure_index = bisect.bisect(machine_failure_cycles, row[\"cycle\"])\n",
    "\n",
    "\n",
    "    if next_failure_index < len(machine_failure_cycles):\n",
    "        \n",
    "        # Calculates the number of cycles left until a failure cycle\n",
    "        return machine_failure_cycles[next_failure_index] - row[\"cycle\"]\n",
    "    else:\n",
    "        \n",
    "        # Between the last known failure of a machine and the end of the analysis\n",
    "        # the time to failure doesn't matter much, so we'll fill it with a mask value of 9999\n",
    "        return 9999\n",
    "    \n",
    "    \n",
    "telemetry[\"ttf\"] = telemetry.apply(lambda row: add_time_to_failure(row, failure_cycles), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the labels\n",
    "\n",
    "In order to predict a failure before it happens we need to lag the information about the failure back a few cycles so our model can learn a failure pattern before it happens.\n",
    "\n",
    "Looking at the data we see that each cycle (an observation of each machine) represents one hour. Given this we might lag our label column 15 cycles back in order to predict failures from at most 15 hours in advance. (__SECALHAR AQUI DEVO RETIRAR TAMBÉM A OBSERVAÇÃO NO MOMENTO DA FALHA PORQUE NÃO ME INTERESSE PREVER QUANDO FALHOU__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
